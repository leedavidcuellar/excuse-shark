# üß™ Manual de Testing con Ollama

## Prerequisitos
```powershell
# 1. Iniciar Ollama
Start-Process -FilePath "C:\Users\lee.cuellar\AppData\Local\Programs\Ollama\ollama.exe" -ArgumentList "serve" -WindowStyle Hidden

# 2. Verificar Ollama
curl http://localhost:11434  # Debe responder: "Ollama is running"

# 3. Iniciar aplicaci√≥n
java -jar target\excusas-shark-1.0.0.jar
```

## Tests Manuales

### Test 1: Endpoint AI Simple
```powershell
curl "http://localhost:8080/api/excuses/ai?context=deployment"
```

**Validaciones**:
- ‚úÖ Responde JSON con 200 OK
- ‚úÖ Contiene campos: `contexto`, `causa`, `consecuencia`, `recomendacion`
- ‚úÖ Los textos son coherentes y relacionados con "deployment"
- ‚úÖ Logs muestran: "Generando excusa con IA. Contexto: deployment"
- ‚ùå Si dice "Usando generaci√≥n cl√°sica como fallback" ‚Üí Ollama no respondi√≥

**Ejemplo de respuesta esperada**:
```json
{
  "id": 1,
  "contexto": "El deploy de producci√≥n se realiz√≥ sin validaci√≥n previa",
  "causa": "El pipeline de CI/CD no ejecut√≥ los tests de integraci√≥n",
  "consecuencia": "Los usuarios experimentaron errores 500 durante 2 horas",
  "recomendacion": "Implementar gates de calidad en el pipeline con tests obligatorios",
  "meme": null,
  "ley": null,
  "roleTarget": null,
  "createdAt": "2025-11-27T19:15:30"
}
```

---

### Test 2: Endpoint AI Ultra (con Meme + Ley)
```powershell
curl "http://localhost:8080/api/excuses/ai/ultra?context=microservicios"
```

**Validaciones**:
- ‚úÖ Responde JSON con 200 OK
- ‚úÖ Contiene campos: `contexto`, `causa`, `consecuencia`, `recomendacion`
- ‚úÖ **ADEM√ÅS** contiene: `meme` (texto gracioso), `ley` (nombre + enunciado)
- ‚úÖ El contexto est√° relacionado con "microservicios"
- ‚úÖ Logs muestran: "Generando excusa con IA. Contexto: microservicios"

**Ejemplo de respuesta esperada**:
```json
{
  "id": 2,
  "contexto": "La arquitectura de microservicios tiene latencia alta",
  "causa": "Falta de circuit breakers entre servicios",
  "consecuencia": "Timeouts en cascada cuando un servicio falla",
  "recomendacion": "Implementar Resilience4j con circuit breakers y fallbacks",
  "meme": "A m√≠ no me gusta trabajar, a m√≠ me gusta cobrar - Ricardo Fort",
  "ley": "MURPHY - Si algo puede salir mal, saldr√° mal",
  "roleTarget": null,
  "createdAt": "2025-11-27T19:16:45"
}
```

---

### Test 3: Diferentes Contextos
Prob√° con distintos contextos para ver c√≥mo se adapta la IA:

```powershell
# Testing
curl "http://localhost:8080/api/excuses/ai?context=testing"

# Base de datos
curl "http://localhost:8080/api/excuses/ai?context=database"

# Performance
curl "http://localhost:8080/api/excuses/ai?context=performance"

# CI/CD
curl "http://localhost:8080/api/excuses/ai?context=ci-cd"

# C√≥digo Legacy
curl "http://localhost:8080/api/excuses/ai?context=codigo-legado"
```

**Validaci√≥n**:
- ‚úÖ Cada respuesta debe estar relacionada con el contexto enviado
- ‚úÖ Los 4 campos deben formar una narrativa coherente
- ‚úÖ No debe repetir las mismas excusas

---

### Test 4: Contexto Vac√≠o (Fallback a default)
```powershell
curl "http://localhost:8080/api/excuses/ai"
```

**Validaci√≥n**:
- ‚úÖ Responde igual, pero usa contexto gen√©rico
- ‚úÖ Logs muestran: "Situaci√≥n general de desarrollo de software"

---

### Test 5: Comparaci√≥n AI vs Classic
Prob√° el endpoint cl√°sico para comparar:

```powershell
curl "http://localhost:8080/api/excuses?context=CODIGO_LEGADO"
```

**Diferencias esperadas**:
- **Classic**: Fragmentos aleatorios de la BD (pueden ser incoherentes entre s√≠)
- **AI**: Narrativa coherente generada por Ollama adaptada al contexto

---

### Test 6: Verificar Logs
Revis√° los logs en tiempo real mientras hac√©s los requests:

```powershell
# Buscar l√≠neas importantes
Select-String -Path ".\logs\spring.log" -Pattern "Generando excusa con IA"
```

**Logs esperados SI Ollama funciona**:
```
INFO : Generando excusa con IA. Contexto: deployment
DEBUG: Respuesta de IA: {"contexto":"...","causa":"..."}
INFO : Excusa generada con IA guardada con ID: 1
```

**Logs esperados SI Ollama falla**:
```
INFO : Generando excusa con IA. Contexto: deployment
ERROR: Error generando excusa con IA: Connect timed out
INFO : Usando generaci√≥n cl√°sica como fallback
```

---

### Test 7: Swagger UI (Interfaz Visual)
Abr√≠ en el navegador:
```
http://localhost:8080/swagger-ui.html
```

1. Expand√≠ **ExcuseController**
2. Prob√° `/api/excuses/ai` y `/api/excuses/ai/ultra`
3. Ingres√° diferentes contextos en "Try it out"
4. Observ√° las respuestas JSON formateadas

---

### Test 8: Stress Test (Ollama bajo carga)
```powershell
# Hacer 10 requests r√°pidos
1..10 | ForEach-Object {
    Write-Host "Request $_"
    curl "http://localhost:8080/api/excuses/ai?context=test$_" | ConvertFrom-Json | Select-Object id, contexto
    Start-Sleep -Seconds 1
}
```

**Validaciones**:
- ‚úÖ Todas las respuestas exitosas (200 OK)
- ‚úÖ Los IDs son √∫nicos y secuenciales
- ‚úÖ Ollama responde consistentemente
- ‚ö†Ô∏è Si alguno falla ‚Üí Verifica logs para ver si hubo timeout

---

### Test 9: Fallback Manual (Detener Ollama)
```powershell
# 1. Detener Ollama
Get-Process -Name ollama -ErrorAction SilentlyContinue | Stop-Process -Force

# 2. Probar endpoint
curl "http://localhost:8080/api/excuses/ai?context=test"
```

**Validaci√≥n**:
- ‚úÖ Debe responder 200 OK (NO error 500)
- ‚úÖ Logs muestran: "Error generando excusa con IA" + "Usando generaci√≥n cl√°sica como fallback"
- ‚úÖ La excusa es v√°lida pero construida con fragmentos random

---

### Test 10: Verificar Base de Datos H2
Abr√≠ en el navegador:
```
http://localhost:8080/h2-console
```

**Configuraci√≥n**:
- JDBC URL: `jdbc:h2:mem:excusasdb`
- User: `sa`
- Password: _(vac√≠o)_

**Query para verificar excusas guardadas**:
```sql
SELECT * FROM EXCUSES ORDER BY CREATED_AT DESC;
```

**Validaciones**:
- ‚úÖ Cada request a `/api/excuses/ai` crea un registro
- ‚úÖ Los campos `contexto`, `causa`, `consecuencia`, `recomendacion` est√°n completos
- ‚úÖ Los registros Ultra tienen `meme` y `ley` tambi√©n

---

## Checklist de Validaci√≥n Completa

### ‚úÖ Funcionalidad
- [ ] Ollama responde en http://localhost:11434
- [ ] App inicia sin errores en puerto 8080
- [ ] Endpoint `/api/excuses/ai` responde 200 OK
- [ ] Endpoint `/api/excuses/ai/ultra` incluye meme + ley
- [ ] Las excusas AI son coherentes con el contexto enviado
- [ ] Las excusas AI no son fragmentos aleatorios (narrativa completa)

### ‚úÖ Resiliencia
- [ ] Si Ollama no responde ‚Üí Fallback funciona (no 500 error)
- [ ] Logs claros indicando uso de fallback
- [ ] La app no se cae si Ollama muere

### ‚úÖ Integraci√≥n
- [ ] Los JSONs tienen todos los 4 campos requeridos
- [ ] Swagger UI funciona correctamente
- [ ] H2 Console muestra registros guardados
- [ ] Diferentes contextos generan respuestas diferentes

### ‚úÖ Coverage
- [ ] 206/206 tests pasando
- [ ] 89% coverage (reportado por JaCoCo)
- [ ] AIResponseParser 100% cubierto

---

## Troubleshooting

### Problema: "Error generando excusa con IA: The template string is not valid"
**Soluci√≥n**: Template tiene comillas incorrectas. Ya arreglado en la versi√≥n actual.

### Problema: "Ollama is not running"
**Soluci√≥n**:
```powershell
Start-Process -FilePath "C:\Users\lee.cuellar\AppData\Local\Programs\Ollama\ollama.exe" -ArgumentList "serve" -WindowStyle Hidden
```

### Problema: "This site can't be reached" (puerto 8080)
**Soluci√≥n**:
```powershell
java -jar target\excusas-shark-1.0.0.jar
```

### Problema: Respuestas muy lentas (>30s)
**Causas posibles**:
- Ollama procesando modelo pesado ‚Üí Normal en primera ejecuci√≥n
- CPU/RAM limitada ‚Üí Configurar timeout m√°s bajo
- Modelo llama3.2 necesita warmup ‚Üí Primera request siempre es lenta

---

## Resultado Esperado

Si TODO funciona correctamente, deber√≠as ver:

**En los logs**:
```
INFO : Started ExcusasSharkApplication in 8.888 seconds
INFO : Generando excusa con IA. Contexto: deployment
DEBUG: Respuesta de IA: {"contexto":"El deploy..."}
INFO : Excusa generada con IA guardada con ID: 1
```

**En la respuesta**:
```json
{
  "id": 1,
  "contexto": "El deploy de producci√≥n...",
  "causa": "Pipeline de CI/CD mal configurado...",
  "consecuencia": "Downtime de 2 horas...",
  "recomendacion": "Implementar smoke tests autom√°ticos...",
  "meme": null,
  "ley": null,
  "roleTarget": null,
  "createdAt": "2025-11-27T19:20:00"
}
```

**Diferencia clave vs fallback**:
- **Con Ollama**: Narrativa coherente, contexto respetado
- **Con Fallback**: Fragmentos random que pueden no tener sentido juntos
