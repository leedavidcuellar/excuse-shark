# ============================================
# Excusas Shark - Configuración de Aplicación
# ============================================

# Puerto de ejecución
server.port=8080

# ============================================
# H2 Database
# ============================================
spring.datasource.url=jdbc:h2:mem:excusasdb
spring.datasource.driverClassName=org.h2.Driver
spring.datasource.username=sa
spring.datasource.password=

# H2 Console (acceder en: http://localhost:8080/h2-console)
spring.h2.console.enabled=true
spring.h2.console.path=/h2-console

# ============================================
# JPA/Hibernate Configuration
# ============================================
spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
spring.jpa.hibernate.ddl-auto=create-drop
spring.jpa.show-sql=false
spring.jpa.properties.hibernate.format_sql=true
spring.jpa.properties.hibernate.use_sql_comments=true

# ============================================
# Logging
# ============================================
logging.level.root=INFO
logging.level.com.excusasshark=DEBUG
logging.level.org.springframework.web=INFO
logging.level.org.hibernate.SQL=DEBUG
logging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACE

# ============================================
# Application Info
# ============================================
spring.application.name=excusas-shark-api
# app metadata (requires Spring Boot Actuator to expose at /actuator/info)
# info.app.version=1.0.0
# info.app.description=API REST para generar excusas técnicas argentinas

# ============================================
# OpenAPI/Swagger
# ============================================
springdoc.api-docs.path=/api-docs
springdoc.swagger-ui.path=/swagger-ui.html
springdoc.swagger-ui.enabled=true
springdoc.swagger-ui.operations-sorter=method

# ============================================
# Spring AI - Ollama Configuration (MEGALODON)
# ============================================
# URL de Ollama (por defecto localhost:11434)
spring.ai.ollama.base-url=http://localhost:11434

# Modelo a usar (llama3.2, mistral, codellama, etc)
# Cambiar según el modelo que tengas en Ollama
spring.ai.ollama.chat.options.model=llama3.2

# Temperatura (creatividad): 0.0 = determinista, 1.0 = muy creativo
spring.ai.ollama.chat.options.temperature=0.7

# Máximo de tokens en la respuesta
spring.ai.ollama.chat.options.max-tokens=1000

# Top P (diversidad de respuestas)
spring.ai.ollama.chat.options.top-p=0.9

