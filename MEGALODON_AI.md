# ü¶àü¶àü¶à NIVEL MEGALODON - Generaci√≥n con IA

## Generaci√≥n de Excusas usando Inteligencia Artificial

El nivel **Megalodon** integra **Spring AI + Ollama** para generar excusas t√©cnicas usando modelos de lenguaje (LLM).

---

## üéØ Caracter√≠sticas

- ‚úÖ **Integraci√≥n con Ollama**: Soporta m√∫ltiples modelos (OpenAI, Claude, Gemini, Llama, Mistral, etc)
- ‚úÖ **Fallback Autom√°tico**: Si falla la IA, usa generaci√≥n cl√°sica
- ‚úÖ **Contexto Personalizado**: Genera excusas espec√≠ficas seg√∫n contexto del usuario
- ‚úÖ **Prompts Optimizados**: Templates dise√±ados para mantener estilo tech argentino
- ‚úÖ **Parsing Robusto**: Extracci√≥n inteligente del JSON de respuesta
- ‚úÖ **Modo ULTRA**: Combina IA + memes + leyes

---

## üì¶ Requisitos

### 1. Instalar Ollama

**Windows/Mac/Linux:**
```bash
# Descargar desde: https://ollama.com/download

# Verificar instalaci√≥n
ollama --version
```

### 2. Descargar Modelo

```bash
# Modelos recomendados:
ollama pull llama3.2      # R√°pido, creativo (3.2B par√°metros)
ollama pull mistral       # Excelente para texto (7B par√°metros)
ollama pull codellama     # Especializado en c√≥digo (7B par√°metros)
ollama pull llama3        # M√°s potente (8B par√°metros)

# Ver modelos instalados
ollama list
```

### 3. Ejecutar Ollama

```bash
# Iniciar servidor (localhost:11434)
ollama serve
```

---

## ‚öôÔ∏è Configuraci√≥n

### application.properties

```properties
# URL de Ollama
spring.ai.ollama.base-url=http://localhost:11434

# Modelo a usar (cambiar seg√∫n disponibilidad)
spring.ai.ollama.chat.options.model=llama3.2

# Temperatura (0.0 = determinista, 1.0 = muy creativo)
spring.ai.ollama.chat.options.temperature=0.7

# M√°ximo tokens en respuesta
spring.ai.ollama.chat.options.max-tokens=1000

# Top P (diversidad de respuestas)
spring.ai.ollama.chat.options.top-p=0.9
```

### Cambiar Modelo en Runtime

Modificar `spring.ai.ollama.chat.options.model` seg√∫n el modelo descargado:
- `llama3.2` - R√°pido, ideal para desarrollo
- `mistral` - Mejor calidad de texto
- `codellama` - Enfocado en c√≥digo y tech
- `llama3` - M√°s completo pero m√°s lento

---

## üîå Endpoints Megalodon

### 1. Excusa con IA

```bash
GET /api/excuses/ai

# Ejemplos:
curl "http://localhost:8080/api/excuses/ai"
curl "http://localhost:8080/api/excuses/ai?context=microservicios"
curl "http://localhost:8080/api/excuses/ai?context=deploy de viernes"
```

**Respuesta:**
```json
{
  "id": 10,
  "contexto": "El microservicio de autenticaci√≥n manejaba 50K requests/segundo",
  "causa": "No se implement√≥ rate limiting ni circuit breakers",
  "consecuencia": "Los costos de AWS subieron 800% en 3 horas",
  "recomendacion": "Implementar Resilience4j con l√≠mites por tenant",
  "meme": null,
  "ley": null,
  "roleTarget": null,
  "createdAt": "2024-01-15T10:40:00"
}
```

### 2. Excusa ULTRA con IA

```bash
GET /api/excuses/ai/ultra

# Ejemplo:
curl "http://localhost:8080/api/excuses/ai/ultra?context=deploy de producci√≥n"
```

**Respuesta:**
```json
{
  "id": 11,
  "contexto": "Deploy en viernes a las 17hs sin testing",
  "causa": "PM insisti√≥ que eran solo 3 l√≠neas de c√≥digo",
  "consecuencia": "Sistema ca√≠do 4 horas, 100K usuarios afectados",
  "recomendacion": "Freeze window los viernes + rollback automatizado",
  "meme": "Mir√° vos... commiteaste la contrase√±a en el repo p√∫blico",
  "ley": "MURPHY - Si algo puede salir mal, saldr√° mal en el peor momento",
  "roleTarget": null,
  "createdAt": "2024-01-15T10:45:00"
}
```

---

## üß† Prompt Engineering

### Template Usado

```
Sos un desarrollador argentino experto en generar excusas t√©cnicas creativas y realistas.

Gener√° una excusa tech siguiendo esta estructura JSON:
{
  "contexto": "Descripci√≥n del escenario problem√°tico",
  "causa": "La raz√≥n t√©cnica del problema",
  "consecuencia": "El impacto del problema",
  "recomendacion": "Soluci√≥n t√©cnica profesional"
}

Contexto adicional: {context}

IMPORTANTE:
- Us√° lenguaje t√©cnico profesional (deployment, pipeline, microservicios, etc)
- Las excusas deben ser coherentes y realistas
- Evit√° humor excesivo en el JSON principal
- Respond√© SOLO con el JSON, sin texto adicional
```

### Personalizaci√≥n

Para modificar el estilo de generaci√≥n, editar:
```java
// AIExcuseGeneratorService.java
private static final String EXCUSE_GENERATION_PROMPT = """
    // Tu prompt personalizado aqu√≠
    """;
```

---

## üõ°Ô∏è Fallback Autom√°tico

Si Ollama no est√° disponible o falla la generaci√≥n:

```java
try {
    // Intenta generar con IA
    return aiExcuseGeneratorService.generateAIExcuse(context);
} catch (Exception e) {
    // Fallback autom√°tico a generaci√≥n cl√°sica
    log.info("Usando generaci√≥n cl√°sica como fallback");
    return fallbackService.generateRandomExcuse();
}
```

**Ventajas:**
- ‚úÖ La API nunca falla
- ‚úÖ Funciona sin Ollama (modo degradado)
- ‚úÖ Logs claros de fallback
- ‚úÖ Tests no requieren Ollama

---

## üß™ Testing

### Test Manual

```bash
# 1. Verificar Ollama activo
curl http://localhost:11434/api/tags

# 2. Generar excusa con IA
curl "http://localhost:8080/api/excuses/ai?context=kubernetes crash"

# 3. Generar ULTRA con IA
curl "http://localhost:8080/api/excuses/ai/ultra?context=incident de producci√≥n"
```

### Test con PowerShell

```powershell
# Test b√°sico
$ai = Invoke-RestMethod "http://localhost:8080/api/excuses/ai"
$ai | Format-List

# Test con contexto
$ai = Invoke-RestMethod "http://localhost:8080/api/excuses/ai?context=docker container crash"
Write-Host "CONTEXTO:" -ForegroundColor Cyan
Write-Host $ai.contexto
Write-Host "`nCAUSA:" -ForegroundColor Magenta  
Write-Host $ai.causa
Write-Host "`nCONSECUENCIA:" -ForegroundColor Red
Write-Host $ai.consecuencia
Write-Host "`nRECOMENDACI√ìN:" -ForegroundColor Green
Write-Host $ai.recomendacion
```

---

## üìä Comparaci√≥n Generaci√≥n Cl√°sica vs IA

| Caracter√≠stica | Cl√°sica | IA (Megalodon) |
|----------------|---------|----------------|
| **Velocidad** | ‚ö° Instant√°nea | üïê 1-3 segundos |
| **Creatividad** | üé≤ Aleatoria | üß† Contextual |
| **Coherencia** | ‚úÖ Garantizada | ‚úÖ Alta (depende del modelo) |
| **Personalizaci√≥n** | ‚ùå Limitada | ‚úÖ Por contexto |
| **Requisitos** | ‚úÖ Ninguno | üì¶ Ollama + modelo |
| **Costo** | üí∞ Gratis | üí∞ Gratis (local) |
| **Fallback** | - | ‚úÖ A cl√°sica |

---

## üöÄ Roadmap Megalodon

### Implementado ‚úÖ
- [x] Integraci√≥n Spring AI + Ollama
- [x] Endpoint `/api/excuses/ai`
- [x] Endpoint `/api/excuses/ai/ultra`
- [x] Fallback a generaci√≥n cl√°sica
- [x] Configuraci√≥n flexible de modelos
- [x] Parsing robusto de JSON
- [x] Logging detallado

### Pr√≥ximas Mejoras üîú
- [ ] Cache de respuestas IA (Redis)
- [ ] Rate limiting por usuario
- [ ] M√©tricas de calidad de respuestas
- [ ] A/B testing IA vs Cl√°sica
- [ ] Fine-tuning de prompts
- [ ] Soporte para streaming de respuestas
- [ ] Tests unitarios con mocks

---

## üêõ Troubleshooting

### Ollama no responde

```bash
# Verificar que Ollama est√© corriendo
curl http://localhost:11434/api/tags

# Si no responde, reiniciar Ollama
ollama serve
```

### Modelo no encontrado

```bash
# Listar modelos instalados
ollama list

# Descargar el modelo configurado
ollama pull llama3.2
```

### Respuestas lentas

```properties
# Reducir max-tokens
spring.ai.ollama.chat.options.max-tokens=500

# Usar modelo m√°s r√°pido
spring.ai.ollama.chat.options.model=llama3.2
```

### Excusas poco coherentes

```properties
# Reducir temperatura (m√°s determinista)
spring.ai.ollama.chat.options.temperature=0.3

# O probar otro modelo
spring.ai.ollama.chat.options.model=mistral
```

---

## üìö Referencias

- **Spring AI**: https://docs.spring.io/spring-ai/reference/
- **Ollama**: https://ollama.com/
- **Modelos disponibles**: https://ollama.com/library
- **Llama 3.2**: https://ollama.com/library/llama3.2
- **Mistral**: https://ollama.com/library/mistral

---

**ü¶àü¶àü¶à Nivel Megalodon Completo - IA + Excusas Tech = üî•**
